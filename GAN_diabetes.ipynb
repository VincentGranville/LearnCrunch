{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a72af1a",
   "metadata": {},
   "source": [
    "<h1>GAN: Synthetizing the Diabetes Dataset - Module 3</h1>\n",
    "<p>\n",
    "This notebook covers Module 3 of my course on synthetic data and generative AI. This module features the GAN method (generative adversarial networks) applied to tabular data. Emphasis is on a creating replicable synthetic data by controlling all sources of randomness in GAN (TensorFlow, Python, Numpy), leveraging the seed to get the best synthetic data, reducing training time, assessing the quality of synthetized data using the distance between correlation matrices, and identifing when GAN performs better than Copulas. A separate GAN may be used to synthetize missing observations.\n",
    "<p>\n",
    "The dataset diabetes.csv is used to predict the risk of cancer based on other features such as BMI, insulin, blood pressure, and so on. The rightmost column is the response: cancer status (yes or no). The dataset is available \n",
    "    <a href=\"https://github.com/VincentGranville/Main/blob/main/diabetes.csv\">here</a>.\n",
    "<p>    \n",
    "The content is as follows:\n",
    "    \n",
    "<ol>\n",
    "    <li><a href=\"#section1\">Imports and Reading Datasets</a>  \n",
    "    <li><a href=\"#section2\">Seed, Replicability and Faster Training</a>\n",
    "    <li><a href=\"#section3\">Supervised Classification: Real Data</a>\n",
    "    <li><a href=\"#section4\">Setting up the Deep Neural Network</a>\n",
    "    <li><a href=\"#section5\">Assessing the Quality of the Synthetized Data</a>\n",
    "    <li><a href=\"#section6\">Main Part</a>\n",
    "    <li><a href=\"#section7\">Supervised Classification: Synthetic Data</a>\n",
    "    <li><a href=\"#section8\">Final Evalution of the Synthetized Data</a>\n",
    "    <li><a href=\"#section9\">Conclusions</a>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24118e9c",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<h2>1. Imports and Reading Dataset</h2><a id='section1'></a>\n",
    "<p>\n",
    "The dataset has the following features: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI\tDiabetesPedigreeFunction, Age, Outcome. \n",
    "    <p>\n",
    "   More information is available in chapter 10 in my book \n",
    "    <em>Synthetic Data and Generative AI</em>, available <a href=\"https://mltechniques.com/shop/\">here</a>.\n",
    "The last feature \"Outcome\" is the response: cancer or not. Values set to zero correspond to missing data, except for the binary response (cancer status). Here I ignore observations with missing values, but they can be processed with a separate GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22c1246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "753            0      181             88             44      510  43.3   \n",
      "755            1      128             88             39      110  36.5   \n",
      "760            2       88             58             26       16  28.4   \n",
      "763           10      101             76             48      180  32.9   \n",
      "765            5      121             72             23      112  26.2   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "753                     0.222   26        1  \n",
      "755                     1.057   37        1  \n",
      "760                     0.766   22        0  \n",
      "763                     0.171   63        0  \n",
      "765                     0.245   30        0  \n",
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "from tensorflow import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam    # type of gradient descent optimizer\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "# rows with missing data must be treated separately: I remove them here\n",
    "data.drop(data.index[(data[\"Insulin\"] == 0)], axis=0, inplace=True) \n",
    "data.drop(data.index[(data[\"Glucose\"] == 0)], axis=0, inplace=True) \n",
    "data.drop(data.index[(data[\"BMI\"] == 0)], axis=0, inplace=True) \n",
    "# no further data transformation used beyond this point\n",
    "data.to_csv('diabetes_clean.csv')\n",
    "\n",
    "print (data.shape)\n",
    "print (data.tail())\n",
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938be07",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "<h2>2. Seed, Replicability and Faster Training</h2>\n",
    "<p>\n",
    "The synthetized data is very sensitive to the seed. Trying different seeds allow you to find one that provides better results, or leads faster to the same quality results. In the enhanced mode, the code allows you to stop the main loop (epoch iterations) as soon as a good enough solution is found. In order to do so, the synthetized data is assessed for its quality every <code>n_eval</code> epochs. By default (standard mode), the number of epochs is set to <code>n_epochs=10000</code>. The learning rate is another critical hyperparameter that you can find tune. See also my article on smart search grid to automatically and efficiently fine tune hyperparameters. The article (with Python code) is available \n",
    "    <a href=\"https://mltechniques.com/2023/03/30/smart-grid-search-case-study-with-hybrid-zeta-geometric-distributions-and-synthetic-data/\">here</a>. Further optimization can be achieved by reducing the size of the training set \n",
    "    (randomly removing more than 50% of all observations, see <a href=\"https://mltechniques.com/2023/04/07/massively-speed-up-your-learning-algorithm-with-stochastic-thinning/\">here</a>) or eliminating redundant features via feature clustering (see <a href=\"https://mltechniques.com/2023/03/12/feature-clustering-a-simple-solution-to-many-machine-learning-problems/\">here</a>).\n",
    "    \n",
    "In addition, my GAN implementation leads to replicable results if you use the same seed each time you run it.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbc8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 103     # to make results replicable\n",
    "np.random.seed(seed)     # for numpy\n",
    "random.set_seed(seed)    # for tensorflow/keras\n",
    "python_random.seed(seed) # for python\n",
    "\n",
    "adam = Adam(learning_rate=0.001) # also try 0.01\n",
    "latent_dim = 10\n",
    "n_inputs   = 9   # number of features\n",
    "n_outputs  = 9   # number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a43bf",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "<h2>3. Supervised Classification: Real Data</h2>\n",
    "<p>\n",
    "The purpose here is to perform classification using random forests, to predict the risk of cancer, using the real dataset. We will later run the same classifier on the synthetic data as well, to see how it compares. Usually, the real data is blended with synthetic observations to produce a larger training set called <em>augmented training set</em>. The idea is that by enriching the original training set, you get a more powerful predictive model (classifier in this case), offering more robustness and able to deal with future observations truly different from those in the original training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac5a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\cygwin64\\tmp\\ipykernel_10132\\927559860.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf_true.fit(X_true_train,y_true_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Accuracy: 0.754\n",
      "Base classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82        80\n",
      "           1       0.64      0.55      0.59        38\n",
      "\n",
      "    accuracy                           0.75       118\n",
      "   macro avg       0.72      0.70      0.71       118\n",
      "weighted avg       0.75      0.75      0.75       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--- STEP 1: Base Accuracy for Real Dataset\n",
    "\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = ['Outcome']  # OutCome column is the label (binary 0/1) \n",
    "X = data[features]\n",
    "y = data[label] \n",
    "\n",
    "# Real data split into train/test dataset for classification with random forest\n",
    "\n",
    "X_true_train, X_true_test, y_true_train, y_true_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "clf_true = RandomForestClassifier(n_estimators=100)\n",
    "clf_true.fit(X_true_train,y_true_train)\n",
    "y_true_pred=clf_true.predict(X_true_test)\n",
    "print(\"Base Accuracy: %5.3f\" % (metrics.accuracy_score(y_true_test, y_true_pred)))\n",
    "print(\"Base classification report:\\n\",metrics.classification_report(y_true_test, y_true_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914f6f4",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "<h2>4. Setting up the Deep Neural Network</h2>\n",
    "<p>\n",
    "This section features all the functions needed to create the architecture, as well as sampling fake (synthetic) data, sampling from the real data, and generating latent data.\n",
    "    <p>\n",
    "The DNN has two components: the generator for synthetization, and the discriminator to compare synthetic against real data. The GAN model is the combined DNN. The discriminator is used to check if the synthetic data can be distinguished from the real data after enought training (iterations), statistically speaking. If not, it means that the synthetic data is good enough and we can stop the iterations (referred to as epochs). \n",
    "    <p>\n",
    "    The DNN essentially performs a gradient descent using the <code>adam</code> method to minimize a loss function, whose parameters are the weights attached to the neurons in the different layers. Here 3 layers are used in both models (the generator and the discriminator):\n",
    "    <code>model.add</code> adds one layer at a time. We use a different loss function for the generator and discriminator. The value of the loss function is one of the elements stored in the object <code>model</code> created by the functions\n",
    "    <code>defined_generator</code> and <code>define_discriminator</code>, and updated by \n",
    "        <code>model.train_on_batch</code> in <a href=\"#section6\">section 6</a>. The value for the loss function, decreasing over time (on average) over successive epochs in case of convergence to a local minimum, is stored at each epoch in the variables\n",
    "        <code>d_loss_real</code>, <code>d_loss_fake</code> and <code>g_loss_fake</code> (see code in <a href=\"#section6\">section 6</a>), respectively for the discriminator (letter d) and the generator (letter g).\n",
    "        <p>\n",
    "            An epoch consists of processing the full data, once. More details are available in my book on synthetic data and generative AI, available <a href=\"https://mltechniques.com/shop/\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede468a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- STEP 2: Generate Synthetic Data \n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples) \n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples) # random N(0,1) data\n",
    "    X = generator.predict(x_input,verbose=0) \n",
    "    y = np.zeros((n_samples, 1))  # class label = 0 for fake data\n",
    "    return X, y\n",
    "\n",
    "def generate_real_samples(n):\n",
    "    X = data.sample(n)   # sample from real data\n",
    "    y = np.ones((n, 1))  # class label = 1 for real data\n",
    "    return X, y\n",
    "\n",
    "def define_generator(latent_dim, n_outputs): \n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu',  kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['mean_absolute_error']) # \n",
    "    return model\n",
    "\n",
    "def define_discriminator(n_inputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False # weights must be set to not trainable\n",
    "    model = Sequential()\n",
    "    model.add(generator) \n",
    "    model.add(discriminator) \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam)  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17616099",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "<h2> 5. Assessing the Quality of the Synthetized Data</h2>\n",
    "<p>\n",
    "This function also produces the final synthetic data. It compares the correlation matrix (correlations between features/response) observed on the real data, with that computed on the synthetic data. The returned value \n",
    "    <code>g_dist</code> is between 0 and 1, with zero being the best fit. Other metrics (besides the correlation distance) are discussed later in <a href=\"#section8\">section 8</a>, using open source libraries. The array <code>data_fake</code> is the output synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9b0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_distance(data, model, latent_dim, nobs_synth): \n",
    "\n",
    "    # generate nobs_synth synthetic rows as X, and return it as data_fake\n",
    "    # also return correlation distance between data_fake and real data\n",
    "\n",
    "    latent_points = generate_latent_points(latent_dim, nobs_synth)  \n",
    "    X = model.predict(latent_points, verbose=0)  \n",
    "    data_fake = pd.DataFrame(data=X,  columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'])\n",
    " \n",
    "    # convert Outcome field to binary 0/1\n",
    "    outcome_mean = data_fake.Outcome.mean()\n",
    "    data_fake['Outcome'] = data_fake['Outcome'] > outcome_mean\n",
    "    data_fake[\"Outcome\"] = data_fake[\"Outcome\"].astype(int)\n",
    "\n",
    "    # compute correlation distance\n",
    "    R_data      = np.corrcoef(data.T) # T for transpose\n",
    "    R_data_fake = np.corrcoef(data_fake.T)\n",
    "    g_dist = np.average(abs(R_data-R_data_fake))\n",
    "    return(g_dist, data_fake) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd0ce5",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "<h2>6. Main Part</h2>\n",
    "<p>\n",
    " The function <code>train</code> is the main function to train both models (discriminator, generator), produce\n",
    "    the synthetic data <code>data_fake</code>, compute the value <code>g_dist</code> of the generator loss function\n",
    "    at the final epoch, and decide when to stop: after <code>n_epochs</code> iterations or (if using the enhanced mode), after producing good enough synthetic data at iteration <code>best_epoch</code>, whichever comes first. The goodness of fit is computed as the distance between the correlation matrices (real versus synthetic data) as discussed in \n",
    "    <a href=\"#section5\">section 5</a>,\n",
    "    with a call to the function <code>gan_distance</code>.\n",
    "    <p>\n",
    "        The <code>train</code> function also saves the value of the loss functions obtained at each iteration (<code>d_history</code> for the discriminator, <code>g_history</code> for the generator) to the file <code>history.txt</code>, to produce a plot later if desired. \n",
    "        <p>\n",
    "            <b> Important note</b>: <br>\n",
    "            In testing mode, set <code>n_epochs</code> to 200 rather than 10,000 (the recommended value for this dataset) when calling the function <code>train</code>. Otherwise you might have to wait quite a bit for the training to complete. The function <code>train</code> displays a message every <code>n_eval</code> epochs to show the progress, showing the current values of the loss functions and the number of epochs completed.            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c1c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, latent_dim, mode, n_epochs=10000, n_batch=128, n_eval=50):   \n",
    "    \n",
    "    # determine half the size of one batch, for updating the  discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_history = [] \n",
    "    g_history = [] \n",
    "    g_dist_history = []\n",
    "    if mode == 'Enhanced':\n",
    "        g_dist_min = 999999999.0  \n",
    "\n",
    "    for epoch in range(0,n_epochs+1): \n",
    "                 \n",
    "        # update discriminator\n",
    "        x_real, y_real = generate_real_samples(half_batch)  # sample from real data\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real) \n",
    "        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # update generator via the discriminator error\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)  # random input for generator\n",
    "        y_gan = np.ones((n_batch, 1))                        # label = 1 for fake samples\n",
    "        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan) \n",
    "        d_history.append(d_loss)\n",
    "        g_history.append(g_loss_fake)\n",
    "\n",
    "        if mode == 'Enhanced': \n",
    "            (g_dist, data_fake) = gan_distance(data, g_model, latent_dim, nobs_synth=400)\n",
    "            if g_dist < g_dist_min and epoch > int(0.75*n_epochs): \n",
    "               g_dist_min = g_dist\n",
    "               best_data_fake = data_fake\n",
    "               best_epoch = epoch\n",
    "        else: \n",
    "            g_dist = -1.0\n",
    "        g_dist_history.append(g_dist)\n",
    "                \n",
    "        if epoch % n_eval == 0: # evaluate the model every n_eval epochs\n",
    "            print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f g_dist=%.3f' % (epoch, d_loss_real, d_loss_fake, d_loss,  g_loss_fake, g_dist))       \n",
    "            plt.subplot(1, 1, 1)\n",
    "            plt.plot(d_history, label='d')\n",
    "            plt.plot(g_history, label='gen')\n",
    "            # plt.show() # un-comment to see the plots\n",
    "            plt.close()\n",
    "       \n",
    "    OUT=open(\"history.txt\",\"w\")\n",
    "    for k in range(len(d_history)):\n",
    "        OUT.write(\"%6.4f\\t%6.4f\\t%6.4f\\n\" %(d_history[k],g_history[k],g_dist_history[k]))\n",
    "    OUT.close()\n",
    "    \n",
    "    if mode == 'Standard':\n",
    "        # best synth data is assumed to be the one produced at last epoch\n",
    "        best_epoch = epoch\n",
    "        (g_dist_min, best_data_fake) = gan_distance(data, g_model, latent_dim, nobs_synth=400)\n",
    "       \n",
    "    return(g_model, best_data_fake, g_dist_min, best_epoch) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5fd5b",
   "metadata": {},
   "source": [
    "Below is the very core of the program. Besides selecting the mode, initializing the models and saving the synthetic data to the\n",
    "file <code>diabetes_synthetic.csv</code>, it consists of one instruction: the call to the <code>train</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e474ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                250       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                1300      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,601\n",
      "Trainable params: 1,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 15)                165       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                480       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 924\n",
      "Trainable params: 924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      ">0, d1=1.391, d2=0.781 d=1.086 g=0.613 g_dist=0.340\n",
      ">50, d1=0.000, d2=0.474 d=0.237 g=1.072 g_dist=0.354\n",
      ">100, d1=0.000, d2=0.843 d=0.422 g=0.643 g_dist=0.380\n",
      ">150, d1=0.018, d2=0.136 d=0.077 g=2.226 g_dist=0.530\n",
      ">200, d1=0.139, d2=0.237 d=0.188 g=1.594 g_dist=0.333\n",
      ">250, d1=0.002, d2=0.051 d=0.026 g=3.090 g_dist=0.542\n",
      ">300, d1=0.004, d2=0.045 d=0.024 g=3.194 g_dist=0.435\n",
      ">350, d1=0.000, d2=0.074 d=0.037 g=3.127 g_dist=0.472\n",
      ">400, d1=0.259, d2=0.128 d=0.193 g=2.397 g_dist=0.418\n",
      ">450, d1=0.014, d2=0.106 d=0.060 g=2.716 g_dist=0.469\n",
      ">500, d1=0.011, d2=0.120 d=0.065 g=2.717 g_dist=0.491\n",
      ">550, d1=0.028, d2=0.094 d=0.061 g=3.004 g_dist=0.465\n",
      ">600, d1=0.025, d2=0.016 d=0.020 g=5.705 g_dist=0.517\n",
      ">650, d1=0.701, d2=0.105 d=0.403 g=4.028 g_dist=0.651\n",
      ">700, d1=0.007, d2=0.009 d=0.008 g=5.655 g_dist=0.463\n",
      ">750, d1=0.007, d2=0.038 d=0.023 g=4.607 g_dist=0.455\n",
      ">800, d1=0.318, d2=0.599 d=0.458 g=2.098 g_dist=0.473\n",
      ">850, d1=0.412, d2=0.086 d=0.249 g=4.083 g_dist=0.558\n",
      ">900, d1=0.680, d2=0.243 d=0.461 g=2.101 g_dist=0.309\n",
      ">950, d1=0.227, d2=0.429 d=0.328 g=5.102 g_dist=0.435\n",
      ">1000, d1=1.003, d2=0.086 d=0.544 g=4.040 g_dist=0.413\n",
      ">1050, d1=1.036, d2=0.158 d=0.597 g=3.331 g_dist=0.458\n",
      ">1100, d1=0.941, d2=0.092 d=0.516 g=3.670 g_dist=0.407\n",
      ">1150, d1=0.315, d2=0.172 d=0.244 g=3.176 g_dist=0.370\n",
      ">1200, d1=0.203, d2=0.483 d=0.343 g=4.110 g_dist=0.402\n",
      ">1250, d1=0.139, d2=0.229 d=0.184 g=3.417 g_dist=0.375\n",
      ">1300, d1=2.238, d2=1.172 d=1.705 g=1.803 g_dist=0.414\n",
      ">1350, d1=0.695, d2=0.341 d=0.518 g=3.340 g_dist=0.311\n",
      ">1400, d1=0.202, d2=0.080 d=0.141 g=3.363 g_dist=0.398\n",
      ">1450, d1=1.914, d2=0.779 d=1.346 g=2.576 g_dist=0.352\n",
      ">1500, d1=0.284, d2=0.141 d=0.212 g=3.818 g_dist=0.402\n",
      ">1550, d1=0.203, d2=0.213 d=0.208 g=3.326 g_dist=0.350\n",
      ">1600, d1=0.674, d2=0.611 d=0.643 g=2.824 g_dist=0.280\n",
      ">1650, d1=2.483, d2=0.176 d=1.329 g=3.931 g_dist=0.336\n",
      ">1700, d1=2.636, d2=1.269 d=1.953 g=2.147 g_dist=0.405\n",
      ">1750, d1=0.443, d2=0.218 d=0.330 g=2.775 g_dist=0.381\n",
      ">1800, d1=2.484, d2=0.735 d=1.610 g=2.039 g_dist=0.372\n",
      ">1850, d1=1.926, d2=0.833 d=1.379 g=2.094 g_dist=0.400\n",
      ">1900, d1=0.425, d2=0.219 d=0.322 g=3.003 g_dist=0.318\n",
      ">1950, d1=0.638, d2=0.602 d=0.620 g=2.122 g_dist=0.369\n",
      ">2000, d1=1.452, d2=0.440 d=0.946 g=2.127 g_dist=0.412\n",
      ">2050, d1=1.434, d2=0.690 d=1.062 g=2.373 g_dist=0.361\n",
      ">2100, d1=1.368, d2=0.480 d=0.924 g=2.165 g_dist=0.418\n",
      ">2150, d1=0.550, d2=0.216 d=0.383 g=3.043 g_dist=0.435\n",
      ">2200, d1=0.847, d2=0.310 d=0.578 g=2.602 g_dist=0.325\n",
      ">2250, d1=1.536, d2=0.445 d=0.991 g=1.823 g_dist=0.371\n",
      ">2300, d1=0.422, d2=0.391 d=0.407 g=1.789 g_dist=0.403\n",
      ">2350, d1=0.574, d2=0.138 d=0.356 g=2.825 g_dist=0.398\n",
      ">2400, d1=0.591, d2=0.313 d=0.452 g=1.891 g_dist=0.389\n",
      ">2450, d1=1.038, d2=0.615 d=0.827 g=1.789 g_dist=0.357\n",
      ">2500, d1=0.307, d2=0.225 d=0.266 g=2.429 g_dist=0.393\n",
      ">2550, d1=0.411, d2=0.302 d=0.357 g=2.706 g_dist=0.336\n",
      ">2600, d1=0.963, d2=0.352 d=0.658 g=1.816 g_dist=0.470\n",
      ">2650, d1=0.647, d2=0.639 d=0.643 g=1.259 g_dist=0.461\n",
      ">2700, d1=0.503, d2=0.429 d=0.466 g=1.776 g_dist=0.433\n",
      ">2750, d1=0.998, d2=0.551 d=0.775 g=1.117 g_dist=0.334\n",
      ">2800, d1=0.688, d2=0.753 d=0.720 g=1.527 g_dist=0.323\n",
      ">2850, d1=0.643, d2=0.617 d=0.630 g=1.363 g_dist=0.306\n",
      ">2900, d1=0.859, d2=0.961 d=0.910 g=1.286 g_dist=0.320\n",
      ">2950, d1=0.889, d2=0.524 d=0.707 g=1.327 g_dist=0.314\n",
      ">3000, d1=0.438, d2=0.814 d=0.626 g=1.176 g_dist=0.319\n",
      ">3050, d1=0.735, d2=0.753 d=0.744 g=1.053 g_dist=0.267\n",
      ">3100, d1=0.848, d2=0.813 d=0.831 g=0.931 g_dist=0.291\n",
      ">3150, d1=0.810, d2=0.397 d=0.604 g=1.395 g_dist=0.250\n",
      ">3200, d1=0.879, d2=0.446 d=0.663 g=1.305 g_dist=0.297\n",
      ">3250, d1=0.646, d2=0.475 d=0.561 g=1.620 g_dist=0.281\n",
      ">3300, d1=0.813, d2=0.677 d=0.745 g=1.201 g_dist=0.290\n",
      ">3350, d1=0.923, d2=0.962 d=0.943 g=0.871 g_dist=0.304\n",
      ">3400, d1=0.597, d2=0.816 d=0.707 g=1.232 g_dist=0.322\n",
      ">3450, d1=0.979, d2=0.704 d=0.842 g=1.028 g_dist=0.302\n",
      ">3500, d1=0.676, d2=0.562 d=0.619 g=1.186 g_dist=0.328\n",
      ">3550, d1=0.670, d2=0.699 d=0.685 g=1.148 g_dist=0.315\n",
      ">3600, d1=0.709, d2=0.396 d=0.552 g=1.598 g_dist=0.362\n",
      ">3650, d1=0.890, d2=0.339 d=0.614 g=1.718 g_dist=0.284\n",
      ">3700, d1=0.220, d2=0.286 d=0.253 g=2.055 g_dist=0.368\n",
      ">3750, d1=0.860, d2=0.710 d=0.785 g=1.168 g_dist=0.274\n",
      ">3800, d1=0.686, d2=0.619 d=0.653 g=1.167 g_dist=0.371\n",
      ">3850, d1=1.136, d2=0.718 d=0.927 g=1.097 g_dist=0.250\n",
      ">3900, d1=0.872, d2=0.981 d=0.927 g=1.048 g_dist=0.348\n",
      ">3950, d1=0.651, d2=0.603 d=0.627 g=1.173 g_dist=0.278\n",
      ">4000, d1=0.495, d2=0.281 d=0.388 g=1.540 g_dist=0.294\n",
      ">4050, d1=0.707, d2=0.674 d=0.690 g=1.098 g_dist=0.380\n",
      ">4100, d1=0.670, d2=0.481 d=0.575 g=1.458 g_dist=0.339\n",
      ">4150, d1=0.382, d2=0.412 d=0.397 g=1.676 g_dist=0.325\n",
      ">4200, d1=0.783, d2=0.562 d=0.673 g=1.574 g_dist=0.334\n",
      ">4250, d1=0.850, d2=0.823 d=0.837 g=1.079 g_dist=0.335\n",
      ">4300, d1=0.579, d2=0.501 d=0.540 g=1.559 g_dist=0.339\n",
      ">4350, d1=0.435, d2=0.362 d=0.399 g=1.452 g_dist=0.364\n",
      ">4400, d1=0.982, d2=0.760 d=0.871 g=1.158 g_dist=0.328\n",
      ">4450, d1=1.049, d2=0.725 d=0.887 g=0.993 g_dist=0.428\n",
      ">4500, d1=0.365, d2=0.499 d=0.432 g=1.568 g_dist=0.436\n",
      ">4550, d1=0.714, d2=0.699 d=0.706 g=1.085 g_dist=0.414\n",
      ">4600, d1=0.458, d2=0.651 d=0.554 g=1.669 g_dist=0.474\n",
      ">4650, d1=0.782, d2=0.702 d=0.742 g=0.995 g_dist=0.424\n",
      ">4700, d1=1.199, d2=0.683 d=0.941 g=1.005 g_dist=0.424\n",
      ">4750, d1=0.770, d2=0.617 d=0.693 g=1.271 g_dist=0.476\n",
      ">4800, d1=0.743, d2=1.104 d=0.923 g=0.772 g_dist=0.425\n",
      ">4850, d1=0.744, d2=0.645 d=0.694 g=1.101 g_dist=0.524\n",
      ">4900, d1=0.642, d2=0.789 d=0.715 g=1.027 g_dist=0.347\n",
      ">4950, d1=0.800, d2=0.550 d=0.675 g=1.198 g_dist=0.379\n",
      ">5000, d1=0.736, d2=0.650 d=0.693 g=1.180 g_dist=0.440\n",
      ">5050, d1=1.184, d2=0.922 d=1.053 g=0.905 g_dist=0.413\n",
      ">5100, d1=0.789, d2=0.657 d=0.723 g=1.040 g_dist=0.433\n",
      ">5150, d1=0.537, d2=0.269 d=0.403 g=1.683 g_dist=0.393\n",
      ">5200, d1=0.559, d2=0.445 d=0.502 g=1.553 g_dist=0.507\n",
      ">5250, d1=0.604, d2=0.778 d=0.691 g=0.891 g_dist=0.449\n",
      ">5300, d1=0.669, d2=0.944 d=0.807 g=1.027 g_dist=0.484\n",
      ">5350, d1=0.619, d2=0.734 d=0.677 g=1.019 g_dist=0.458\n",
      ">5400, d1=0.789, d2=1.445 d=1.117 g=0.657 g_dist=0.362\n",
      ">5450, d1=0.991, d2=0.749 d=0.870 g=1.125 g_dist=0.513\n",
      ">5500, d1=1.307, d2=1.184 d=1.246 g=0.633 g_dist=0.522\n",
      ">5550, d1=0.898, d2=1.289 d=1.094 g=0.669 g_dist=0.410\n",
      ">5600, d1=0.799, d2=0.978 d=0.888 g=0.790 g_dist=0.424\n",
      ">5650, d1=1.163, d2=0.681 d=0.922 g=1.191 g_dist=0.359\n",
      ">5700, d1=0.766, d2=0.490 d=0.628 g=1.244 g_dist=0.492\n",
      ">5750, d1=0.953, d2=1.093 d=1.023 g=0.925 g_dist=0.388\n",
      ">5800, d1=0.892, d2=0.632 d=0.762 g=1.092 g_dist=0.515\n",
      ">5850, d1=0.636, d2=0.407 d=0.521 g=1.276 g_dist=0.444\n",
      ">5900, d1=0.505, d2=0.397 d=0.451 g=1.610 g_dist=0.465\n",
      ">5950, d1=0.643, d2=0.633 d=0.638 g=0.986 g_dist=0.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">6000, d1=1.001, d2=0.796 d=0.898 g=0.833 g_dist=0.441\n",
      ">6050, d1=0.773, d2=0.846 d=0.810 g=0.905 g_dist=0.507\n",
      ">6100, d1=1.284, d2=0.861 d=1.072 g=0.824 g_dist=0.406\n",
      ">6150, d1=0.968, d2=0.889 d=0.928 g=0.819 g_dist=0.465\n",
      ">6200, d1=0.759, d2=1.813 d=1.286 g=0.567 g_dist=0.344\n",
      ">6250, d1=0.687, d2=0.740 d=0.713 g=1.018 g_dist=0.549\n",
      ">6300, d1=0.378, d2=0.320 d=0.349 g=1.753 g_dist=0.425\n",
      ">6350, d1=0.691, d2=0.544 d=0.617 g=1.062 g_dist=0.454\n",
      ">6400, d1=0.922, d2=0.950 d=0.936 g=0.759 g_dist=0.485\n",
      ">6450, d1=0.912, d2=0.833 d=0.873 g=0.967 g_dist=0.358\n",
      ">6500, d1=0.599, d2=0.595 d=0.597 g=1.082 g_dist=0.539\n",
      ">6550, d1=0.558, d2=0.476 d=0.517 g=1.494 g_dist=0.363\n",
      ">6600, d1=0.577, d2=0.718 d=0.647 g=1.034 g_dist=0.478\n",
      ">6650, d1=0.611, d2=0.833 d=0.722 g=0.791 g_dist=0.407\n",
      ">6700, d1=0.567, d2=1.066 d=0.816 g=0.773 g_dist=0.355\n",
      ">6750, d1=0.720, d2=0.731 d=0.725 g=1.103 g_dist=0.503\n",
      ">6800, d1=0.648, d2=0.715 d=0.681 g=1.242 g_dist=0.361\n",
      ">6850, d1=0.407, d2=0.448 d=0.427 g=1.299 g_dist=0.495\n",
      ">6900, d1=0.477, d2=0.677 d=0.577 g=1.138 g_dist=0.358\n",
      ">6950, d1=0.899, d2=0.981 d=0.940 g=0.805 g_dist=0.508\n",
      ">7000, d1=0.584, d2=0.559 d=0.572 g=1.387 g_dist=0.569\n",
      ">7050, d1=0.655, d2=0.913 d=0.784 g=0.908 g_dist=0.452\n",
      ">7100, d1=0.282, d2=0.324 d=0.303 g=1.412 g_dist=0.541\n",
      ">7150, d1=0.519, d2=0.618 d=0.568 g=1.167 g_dist=0.398\n",
      ">7200, d1=0.755, d2=0.816 d=0.785 g=0.872 g_dist=0.556\n",
      ">7250, d1=1.246, d2=0.749 d=0.998 g=0.933 g_dist=0.521\n",
      ">7300, d1=0.451, d2=0.474 d=0.463 g=1.466 g_dist=0.435\n",
      ">7350, d1=0.409, d2=0.517 d=0.463 g=1.434 g_dist=0.437\n",
      ">7400, d1=0.675, d2=1.149 d=0.912 g=0.759 g_dist=0.469\n",
      ">7450, d1=1.143, d2=0.885 d=1.014 g=0.851 g_dist=0.351\n",
      ">7500, d1=1.014, d2=0.708 d=0.861 g=0.840 g_dist=0.522\n",
      ">7550, d1=0.499, d2=0.475 d=0.487 g=1.305 g_dist=0.442\n",
      ">7600, d1=0.341, d2=0.315 d=0.328 g=1.712 g_dist=0.385\n",
      ">7650, d1=0.725, d2=0.686 d=0.706 g=0.965 g_dist=0.336\n",
      ">7700, d1=1.351, d2=1.244 d=1.297 g=0.733 g_dist=0.380\n",
      ">7750, d1=0.566, d2=0.400 d=0.483 g=1.519 g_dist=0.478\n",
      ">7800, d1=0.628, d2=0.390 d=0.509 g=1.336 g_dist=0.425\n",
      ">7850, d1=0.449, d2=0.355 d=0.402 g=1.368 g_dist=0.416\n",
      ">7900, d1=0.675, d2=0.566 d=0.621 g=1.299 g_dist=0.298\n",
      ">7950, d1=0.950, d2=0.996 d=0.973 g=0.716 g_dist=0.391\n",
      ">8000, d1=1.291, d2=0.364 d=0.827 g=1.241 g_dist=0.395\n",
      ">8050, d1=0.691, d2=0.589 d=0.640 g=1.136 g_dist=0.405\n",
      ">8100, d1=0.476, d2=0.493 d=0.484 g=1.177 g_dist=0.432\n",
      ">8150, d1=0.508, d2=0.380 d=0.444 g=1.506 g_dist=0.407\n",
      ">8200, d1=1.066, d2=0.469 d=0.768 g=1.210 g_dist=0.311\n",
      ">8250, d1=1.000, d2=0.894 d=0.947 g=1.008 g_dist=0.417\n",
      ">8300, d1=0.609, d2=0.569 d=0.589 g=1.111 g_dist=0.312\n",
      ">8350, d1=0.593, d2=0.334 d=0.463 g=1.445 g_dist=0.492\n",
      ">8400, d1=0.695, d2=0.749 d=0.722 g=1.125 g_dist=0.367\n",
      ">8450, d1=0.829, d2=0.732 d=0.780 g=1.028 g_dist=0.414\n",
      ">8500, d1=0.942, d2=1.066 d=1.004 g=1.097 g_dist=0.448\n",
      ">8550, d1=0.399, d2=0.415 d=0.407 g=1.581 g_dist=0.420\n",
      ">8600, d1=0.847, d2=0.517 d=0.682 g=1.175 g_dist=0.373\n",
      ">8650, d1=0.570, d2=0.361 d=0.466 g=1.625 g_dist=0.484\n",
      ">8700, d1=0.494, d2=0.495 d=0.495 g=1.453 g_dist=0.358\n",
      ">8750, d1=1.006, d2=0.684 d=0.845 g=0.900 g_dist=0.404\n",
      ">8800, d1=0.531, d2=0.831 d=0.681 g=1.023 g_dist=0.454\n",
      ">8850, d1=0.415, d2=0.323 d=0.369 g=1.748 g_dist=0.437\n",
      ">8900, d1=0.857, d2=0.669 d=0.763 g=1.103 g_dist=0.395\n",
      ">8950, d1=1.263, d2=1.045 d=1.154 g=0.842 g_dist=0.457\n",
      ">9000, d1=0.794, d2=0.744 d=0.769 g=1.279 g_dist=0.353\n",
      ">9050, d1=0.512, d2=0.622 d=0.567 g=1.400 g_dist=0.422\n",
      ">9100, d1=0.693, d2=0.646 d=0.669 g=1.158 g_dist=0.382\n",
      ">9150, d1=0.601, d2=1.159 d=0.880 g=0.789 g_dist=0.317\n",
      ">9200, d1=0.395, d2=0.398 d=0.396 g=1.414 g_dist=0.480\n",
      ">9250, d1=0.424, d2=0.542 d=0.483 g=1.273 g_dist=0.372\n",
      ">9300, d1=0.757, d2=0.864 d=0.810 g=0.834 g_dist=0.382\n",
      ">9350, d1=1.310, d2=0.842 d=1.076 g=0.874 g_dist=0.402\n",
      ">9400, d1=0.499, d2=0.486 d=0.493 g=1.573 g_dist=0.384\n",
      ">9450, d1=0.752, d2=0.757 d=0.755 g=1.068 g_dist=0.322\n",
      ">9500, d1=1.275, d2=0.828 d=1.051 g=0.925 g_dist=0.397\n",
      ">9550, d1=0.301, d2=0.326 d=0.313 g=1.821 g_dist=0.415\n",
      ">9600, d1=0.546, d2=0.551 d=0.549 g=1.337 g_dist=0.409\n",
      ">9650, d1=1.301, d2=0.835 d=1.068 g=0.845 g_dist=0.396\n",
      ">9700, d1=1.049, d2=0.774 d=0.911 g=1.025 g_dist=0.319\n",
      ">9750, d1=0.401, d2=0.455 d=0.428 g=1.421 g_dist=0.434\n",
      ">9800, d1=1.235, d2=0.857 d=1.046 g=0.874 g_dist=0.364\n",
      ">9850, d1=1.140, d2=0.621 d=0.880 g=1.101 g_dist=0.407\n",
      ">9900, d1=0.442, d2=0.443 d=0.443 g=1.219 g_dist=0.429\n",
      ">9950, d1=0.679, d2=0.734 d=0.706 g=1.117 g_dist=0.372\n",
      ">10000, d1=0.861, d2=0.775 d=0.818 g=0.846 g_dist=0.289\n"
     ]
    }
   ],
   "source": [
    "#--- main part for building & training model\n",
    "\n",
    "discriminator = define_discriminator(n_inputs)\n",
    "discriminator.summary()\n",
    "generator = define_generator(latent_dim, n_outputs)\n",
    "generator.summary()\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "\n",
    "mode = 'Enhanced'  # options: 'Standard' or 'Enhanced'\n",
    "model, data_fake, g_dist, best_epoch = train(generator, discriminator, gan_model, latent_dim, mode)\n",
    "data_fake.to_csv('diabetes_synthetic.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e5b77",
   "metadata": {},
   "source": [
    "Now plotting sample historical data extracted from the output file <code>history.txt</code> (values of the loss function), starting with two different seeds. It shows the sensitivity to the seed, and the volatility during successive epochs. The right plot indicates that seed 103 is better than 102: it leads to a lower local minimum of the loss function, in fewer iterations (about 6000), compared to seed 102 which produces worse results (higher minimum, higher volatility) and has not fully stabilized even after 8000 epochs. The loss function values of interest are in <code>g_history</code> (attached to the generator model). \n",
    "<p>\n",
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/VincentGranville/Notebooks/main/history.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87051373",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "<h2>7. Supervised Classification: Synthetic Data</h2>\n",
    "<p>\n",
    "Here we assess the performance of the classifier on the synthetic data. That is, we compare the performance statistics with those obtained for the classification on the real data \n",
    "    in section <a href=\"#section3\">section 3</a>. With well synthetized data (as measured via the <code>gan_distance</code> function in <a href=\"#section5\">section 5</a>, or other metrics at the end of this notebook), the classifier does a much better job classifying the synthetic data (assigning cancer status yes/no to each observation), than on the real data.\n",
    "    <p>\n",
    "        In test mode, when using only 200 epochs, the opposite is true: you are far from a good synthetization, and the classifier on the synthetic data is quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c177f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\cygwin64\\tmp\\ipykernel_10132\\567537292.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf_fake.fit(X_fake_train,y_fake_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fake data model: 0.942\n",
      "Classification report of fake data model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        70\n",
      "           1       0.92      0.94      0.93        50\n",
      "\n",
      "    accuracy                           0.94       120\n",
      "   macro avg       0.94      0.94      0.94       120\n",
      "weighted avg       0.94      0.94      0.94       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--- STEP 3: Classify synthetic data based on Outcome field\n",
    "\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = ['Outcome']\n",
    "X_fake_created = data_fake[features]\n",
    "y_fake_created = data_fake[label]\n",
    "X_fake_train, X_fake_test, y_fake_train, y_fake_test = train_test_split(X_fake_created, y_fake_created, test_size=0.30, random_state=42)\n",
    "clf_fake = RandomForestClassifier(n_estimators=100)\n",
    "clf_fake.fit(X_fake_train,y_fake_train)\n",
    "y_fake_pred=clf_fake.predict(X_fake_test)\n",
    "print(\"Accuracy of fake data model: %5.3f\" % (metrics.accuracy_score(y_fake_test, y_fake_pred)))\n",
    "print(\"Classification report of fake data model:\\n\",metrics.classification_report(y_fake_test, y_fake_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab79b2a",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "<h2>8. Final Evalution of the Synthetized Data</h2>\n",
    "<p>\n",
    "    Here I use the <code>table_evaluator</code> Python library to gather several metrics measuring the quality of the synthetized data, in addition to my correlation matrix distance. The \"Basic Statistics\" is an aggregate of multiple distances, such as normalized mean, variance, 25- and 75-percentiles differences (in absolute value) for each feature, between the synthetic and real data. It is equal to 1 if both datasets are identical. Note that the best fit may lead to overfitting, also reproducing the biases present in the real data. Bias removal is best achieved by eliminating features favoring biases, such as race or gender, and replacing them with other or new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7132941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\table_evaluator\\metrics.py:95: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  distances = Parallel(n_jobs=-1)(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier F1-scores and their Jaccard similarities::\n",
      "                             f1_real  f1_fake  jaccard_similarity\n",
      "index                                                            \n",
      "DecisionTreeClassifier_fake   0.4810   0.8481              0.2344\n",
      "DecisionTreeClassifier_real   0.7089   0.6835              0.5048\n",
      "LogisticRegression_fake       0.6329   0.9620              0.4234\n",
      "LogisticRegression_real       0.7595   0.5949              0.4107\n",
      "MLPClassifier_fake            0.6456   0.8861              0.4364\n",
      "MLPClassifier_real            0.6709   0.6329              0.4906\n",
      "RandomForestClassifier_fake   0.6709   0.8608              0.4906\n",
      "RandomForestClassifier_real   0.7975   0.5949              0.5048\n",
      "\n",
      "Privacy results:\n",
      "                                         result\n",
      "Duplicate rows between sets (real/fake)  (0, 0)\n",
      "nearest neighbor mean                    1.6491\n",
      "nearest neighbor std                     0.6970\n",
      "\n",
      "Miscellaneous results:\n",
      "                                  Result\n",
      "Column Correlation Distance RMSE  0.3555\n",
      "Column Correlation distance MAE   0.2833\n",
      "\n",
      "Results:\n",
      "                                                result\n",
      "Basic statistics                                0.9671\n",
      "Correlation column correlations                 0.3836\n",
      "Mean Correlation between fake and real columns  0.9540\n",
      "1 - MAPE Estimator results                      0.6873\n",
      "Similarity Score                                0.7480\n",
      "Avg correlation distance: 0.281\n",
      "Based on epoch number:  7977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\table_evaluator\\metrics.py:121: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  distances = Parallel(n_jobs=-1)(\n"
     ]
    }
   ],
   "source": [
    "#--- STEP 4: Evaluate the Quality of Generated Fake Data With g_dist and Table_evaluator\n",
    "\n",
    "from table_evaluator import load_data, TableEvaluator\n",
    "\n",
    "table_evaluator = TableEvaluator(data, data_fake)\n",
    "table_evaluator.evaluate(target_col='Outcome')\n",
    "# table_evaluator.visual_evaluation() \n",
    "\n",
    "print(\"Avg correlation distance: %5.3f\" % (g_dist))\n",
    "print(\"Based on epoch number: %5d\" % (best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5877b",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "<h2>9. Conclusions</h2>\n",
    "<p>\n",
    "The copula method outperforms GAN on the type of data investigated here, if the criterion to measure quality is how well the correlation structure is reproduced in the synthetic data. See the picture below, comparing the two methods on the diabetes dataset, in terms of correlation matrix. The synth2 matrix corresponds to the GAN-generated synthetic data. The full analysis is in my spreadsheet <code>diabetes.synthetic.xlsx</code> on GitHub,\n",
    "    <a href=\"https://github.com/VincentGranville/Main\">here</a>\n",
    "    <p>\n",
    "    <img  src=\"https://raw.githubusercontent.com/VincentGranville/Notebooks/main/GAN_compare_diabetes.png\" width=600>\n",
    "    <p>\n",
    "        \n",
    "        \n",
    "However, when structures other than linear are present in the real dataset, GAN can do a better job. For instance, in my artificial circle dataset (concentric circles, zero correlation), the copula method correctly generates non-correlated data, but fails to capture the circular structure. GAN does a much better job, as shown in the picture below. The full details are in my spreadsheet <code>Circle8D.xlsc</code>, in the same GitHub folder.\n",
    "        <a href=\"https://github.com/VincentGranville/Main\">here</a>\n",
    "    <p>\n",
    "    <img  src=\"https://raw.githubusercontent.com/VincentGranville/Notebooks/main/gan_circle.png\" width=600>\n",
    "    <p>\n",
    "        \n",
    "The left, middle and right plots correspond respectively to the real data , the copula synthetization, and the GAN synthetization. Actually, it is a 2-dimensional projection of the full 8-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d033d",
   "metadata": {},
   "source": [
    "I discussed in <a href=\"#section2\">section 2</a> different options to train GAN much faster. \n",
    "Another option is to use a fast gradient descent algorithm such as lightGBM. This is implemented in the TabGAN library.\n",
    "Finally, the SDV Python library offers a comprehensive set of synthetization methods, including for tabular data, as well as a variety of real-life datasets. See my sample code \n",
    "<code>GAN_copula_SDV.py</code> in the same GitHub folder. SDV also includes mechanisms to mask private information, using \n",
    "functions from the Faker Python library. It also handles missing data.\n",
    "<p>\n",
    "    <b> Exercise 1</b><br>\n",
    "Blend the synthetic diabetes data with the real data to produce a larger (augmented) training set. Use it to classify the real data into two categories: cancer/no-cancer. Assess the impact of adding synthetic data on the predictions, using cross-validation techniques.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
