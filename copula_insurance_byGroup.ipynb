{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98253bb0",
   "metadata": {},
   "source": [
    "<h1>Copulas: Synthetizing the Insurance Dataset - Module 2</h1>\n",
    "<p>\n",
    "Here we create a separate copula for each group. We then look at performance metrics to assess the quality of synthetic data, comparing the single-copula method with a customized copula for each group. I then discuss in more details parametric copulas, when empirical quantiles are replaced by a parametric distribution, with parameters estimated on the real data. The content is as follows:\n",
    "<ol>\n",
    "    <li><a href=\"#section1\">Imports and reading the data</a>\n",
    "    <li><a href=\"#section2\">Creating the groups, copula for each group, and synthetic data</a>\n",
    "    <li><a href=\"#section3\">Doing the same as in section 2, but without grouping</a>\n",
    "    <li><a href=\"#section4\">Assessing quality of synthetized data</a>\n",
    "    <li><a href=\"#section5\">Alternatives to Hellinger distance</a>\n",
    "    <li><a href=\"#section6\">Parametric copula</a>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adbc93",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<h2>1. Imports and reading the data</h2>\n",
    "<p>\n",
    "The data is stored as a csv file in the local directory. It has the following features: age, sex, bmi, children, smoker, region, charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444f8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90de3ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n",
      "5   31  female  25.740         0     no  southeast   3756.62160\n",
      "6   46  female  33.440         1     no  southeast   8240.58960\n",
      "7   37  female  27.740         3     no  northwest   7281.50560\n",
      "8   37    male  29.830         2     no  northeast   6406.41070\n",
      "9   60  female  25.840         0     no  northwest  28923.13692\n"
     ]
    }
   ],
   "source": [
    "# source: https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset\n",
    "# Fields: age, sex, bmi, children, smoker, region, charges\n",
    "    \n",
    "url=\"https://raw.githubusercontent.com/VincentGranville/Main/main/insurance.csv\"\n",
    "# make sure fields don't contain commas\n",
    "data = pd.read_csv(url)\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bde65",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "<h2>2. Creating the groups, copula for each group, and synthetic data</h2>\n",
    "<p>\n",
    "First we create the group structure. The hash table <code>groupReal</code> containing the real data (organized by group) is indexed by key = (group, idx) where \n",
    "<ul>\n",
    "<li>idx is the index (number) of an observation in the \"group\" in question. Each observation is a list consisting of 4 values: age, bmi, number of children and charges. Thus an observation is the value in the key-value pair that defines the hash table.\n",
    "<li>group is a tab-separated index consisting of gender (male/female), smoking status (yes/no) and region (NorthEast, NorthWest, SouthEast, SouthWest).\n",
    "</ul>\n",
    "The <code>groupCount</code> hash table stores the number of observations in each group. Later on, \n",
    " in the function <code>gaussian_to_synth</code>, I create the hash table\n",
    "<code>groupSynth</code> that contains the synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb94d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupCount = {}\n",
    "groupReal = {}\n",
    "for k in range(0, len(data)):  \n",
    "    obs = data.iloc[k]   # get observation number k\n",
    "    group = obs[1] +\"\\t\"+obs[4]+\"\\t\"+obs[5]\n",
    "    if group in groupCount:\n",
    "        cnt = groupCount[group]\n",
    "        groupReal[(group,cnt)]=(obs[0],obs[2],obs[3],obs[6]) \n",
    "        groupCount[group] += 1    \n",
    "    else:\n",
    "        groupReal[(group,0)]=(obs[0],obs[2],obs[3],obs[6]) \n",
    "        groupCount[group] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18f228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female\tyes\tsouthwest 21\n",
      "male\tno\tsoutheast 134\n",
      "male\tno\tnorthwest 132\n",
      "female\tno\tsoutheast 139\n",
      "female\tno\tnorthwest 135\n",
      "male\tno\tnortheast 125\n",
      "female\tyes\tsoutheast 36\n",
      "male\tno\tsouthwest 126\n",
      "male\tyes\tsoutheast 55\n",
      "female\tno\tnortheast 132\n",
      "male\tyes\tsouthwest 37\n",
      "female\tno\tsouthwest 141\n",
      "female\tyes\tnortheast 29\n",
      "male\tyes\tnortheast 38\n",
      "male\tyes\tnorthwest 29\n",
      "female\tyes\tnorthwest 29\n"
     ]
    }
   ],
   "source": [
    "for group in groupCount:\n",
    "    print(group, groupCount[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc21972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 27.9, 0, 16884.924)\n",
      "(37, 34.8, 2, 39836.519)\n",
      "(64, 31.3, 2, 47291.055)\n",
      "(19, 28.3, 0, 17081.08)\n",
      "(19, 34.7, 2, 36397.576)\n"
     ]
    }
   ],
   "source": [
    "print(groupReal[(\"female\\tyes\\tsouthwest\",0)])\n",
    "print(groupReal[(\"female\\tyes\\tsouthwest\",1)])\n",
    "print(groupReal[(\"female\\tyes\\tsouthwest\",2)])\n",
    "print(groupReal[(\"female\\tyes\\tsouthwest\",3)])\n",
    "print(groupReal[(\"female\\tyes\\tsouthwest\",20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d159c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(group, groupCount, groupReal):\n",
    "\n",
    "    # extract data corresponding to specific group, from big table groupReal\n",
    "\n",
    "    nobs = groupCount[group]\n",
    "    age = []\n",
    "    bmi = []\n",
    "    children = []\n",
    "    charges = []\n",
    "    for cnt in range(nobs):\n",
    "        features = groupReal[(group,cnt)]\n",
    "        age.append(float(features[0]))       # uniform outside very young or very old\n",
    "        bmi.append(float(features[1]))       # Gaussian distribution?\n",
    "        children.append(float(features[2]))  # geometric distribution?\n",
    "        charges.append(float(features[3]))   # bimodal, not gaussian \n",
    "        real = np.stack((age, bmi, children, charges), axis = 0)\n",
    "    return(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36b6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_to_synth(real, gfg, group, nobs_synth, groupSynth):\n",
    "\n",
    "    # turn multivariate gaussian gfg into synth. data, update groupSynth \n",
    "    # this is done for a specific group, creating nobs_synth obs.\n",
    "\n",
    "    age = real[0,:]\n",
    "    bmi = real[1,:]\n",
    "    children = real[2,:]\n",
    "    charges = real[3,:]\n",
    "\n",
    "    g_age = gfg[:,0]\n",
    "    g_bmi = gfg[:,1]\n",
    "    g_children = gfg[:,2]\n",
    "    g_charges = gfg[:,3]\n",
    "\n",
    "    for k in range(nobs_synth):   \n",
    "\n",
    "        u_age = norm.cdf(g_age[k])                     # u stands for uniform[0, 1]\n",
    "        u_bmi = norm.cdf(g_bmi[k])\n",
    "        u_children = norm.cdf(g_children[k])\n",
    "        u_charges = norm.cdf(g_charges[k])\n",
    "\n",
    "        s_age = np.quantile(age, u_age)                # synthesized age \n",
    "        s_bmi = np.quantile(bmi, u_bmi)                # synthesized bmi\n",
    "        s_children = np.quantile(children, u_children) # synthesized children\n",
    "        s_charges = np.quantile(charges, u_charges)    # synthesized charges\n",
    "\n",
    "        # add k-th synth. obs. for group in question, to groupSynth\n",
    "        groupSynth[(group, k)] = [s_age, s_bmi, s_children, s_charges] \n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c49cd",
   "metadata": {},
   "source": [
    "The parameter <code>seed</code> below allows for replicability. Also the quality of the synthetic data may depend on the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e049cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 453\n",
    "np.random.seed(seed)\n",
    "groupSynth = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aec8cb",
   "metadata": {},
   "source": [
    "This is the core of the algorithm, with a loop over each group. The final synthetic data <code>groupSynth</code> is updated in this loop, one group at a time. The array <code>real</code> is a temporary table containing the real data for the group in question, in a simple format to easily compute the correlation matrix <code>corr</code> for the group in question. This matrix is used to produce the Gaussian multivariate vector which is used in turn to produce the synthetic data. The number of synthetic observations produced for each group is <code>nobs_synth</code>. Here, it matches the corresponding number in the real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43a2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groupCount:\n",
    "\n",
    "    real = create_table(group, groupCount, groupReal) \n",
    "    n_var = real.shape[0] \n",
    "    zero = np.zeros(n_var) \n",
    "    corr = np.corrcoef(real)       # correlation matrix for Gaussian copula for this group\n",
    "    nobs_synth = groupCount[group] # number of synthetic obs to create for this group\n",
    "    gfg = np.random.multivariate_normal(zero, corr, nobs_synth) \n",
    "    gaussian_to_synth(real, gfg, group, nobs_synth, groupSynth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68e5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29.24   31.05   2.00   31812.65\n",
      " 37.79   27.89   2.00   27595.47\n",
      " 36.72   26.53   1.00   18972.35\n",
      " 19.25   21.83   2.00   16925.74\n",
      " 19.00   30.78   0.00   17545.09\n",
      " 46.02   30.97   2.00   26974.52\n",
      " 19.00   27.18   0.00   17380.77\n",
      " 29.27   26.80   0.00   18936.03\n",
      " 48.07   31.40   1.00   40524.87\n",
      " 19.85   21.70   1.18   14858.06\n",
      " 19.00   28.15   1.25   17297.46\n",
      " 36.06   31.56   0.00   34401.89\n",
      " 19.00   35.93   0.00   45531.59\n",
      " 46.72   33.87   2.00   42533.74\n",
      " 45.13   33.82   0.00   47295.20\n",
      " 63.04   29.14   0.98   47258.66\n",
      " 38.82   34.66   0.00   47673.08\n",
      " 20.20   22.19   0.00   16970.50\n",
      " 26.66   22.27   2.00   19484.48\n",
      " 23.09   27.62   0.39   18924.28\n",
      " 39.16   31.01   2.00   34694.34\n"
     ]
    }
   ],
   "source": [
    "for group, k in groupSynth:\n",
    "\n",
    "    obs = groupSynth[(group,k)] # this is k-th synth. obs. for group in question\n",
    "\n",
    "    # print synth. data for sample group: age, bmi, children, charges\n",
    "    if group == \"female\\tyes\\tsouthwest\":\n",
    "        print(\"%6.2f %7.2f %6.2f %10.2f\" % (obs[0], obs[1], obs[2], obs[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0e73a",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "<h2>3. Doing the same as in section 2, but without grouping</h2>\n",
    "<p>\n",
    "The goal is to re-use the same coding architecture but this time assuming there is one single group that contains all the data. The purpose is to compare performance metrics (quality of synthetization) obtained via grouping (a separate copula for each group), versus no grouping (one global copula).\n",
    "<p>\n",
    "    New main tables are produced: <code>nogroupSynth</code>, <code>nogroupReal</code> and <code>nogroupCount</code>. They are the equivalent of <code>groupSynth</code>, <code>groupReal</code> and \n",
    "    <code>groupCount</code> used when working with actual groups. They represent respectively the synthesized data, the real data, and the group structure. Since there is no group - or in other words a single global group - iterations over the groups are reduced to just one iteration, making the loop useless.  The \"by group\" loops are kept only for compatibility with the \"multiple groups\" case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d887e9",
   "metadata": {},
   "source": [
    "Now creating the single-group structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdde43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nogroupCount = {}\n",
    "nogroupReal = {}\n",
    "for k in range(0, len(data)):  \n",
    "    obs = data.iloc[k]   # get observation number k\n",
    "    group = \"nogroup\"\n",
    "    if group in nogroupCount:\n",
    "        cnt = nogroupCount[group]\n",
    "        nogroupReal[(group,cnt)]=(obs[0],obs[2],obs[3],obs[6]) \n",
    "        nogroupCount[group] += 1    \n",
    "    else:\n",
    "        nogroupReal[(group,0)]=(obs[0],obs[2],obs[3],obs[6]) \n",
    "        nogroupCount[group] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7b37d",
   "metadata": {},
   "source": [
    "Producing the synthetized data <code>nogroupSynth</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4e9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "nogroupSynth = {}\n",
    "\n",
    "for group in nogroupCount:\n",
    "\n",
    "    real = create_table(group, nogroupCount, nogroupReal) \n",
    "    n_var = real.shape[0] \n",
    "    zero = np.zeros(n_var) \n",
    "    corr = np.corrcoef(real)       # correlation matrix for Gaussian copula for this group\n",
    "    nobs_synth = nogroupCount[group] # number of synthetic obs to create for this group\n",
    "    gfg = np.random.multivariate_normal(zero, corr, nobs_synth) \n",
    "    gaussian_to_synth(real, gfg, group, nobs_synth, nogroupSynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ae00d",
   "metadata": {},
   "source": [
    "Showing the first 10 synthetized observations, with features age, bmi, children, charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88add4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59.00   33.24   2.00   16096.82\n",
      " 34.00   28.12   3.00    9871.41\n",
      " 61.00   25.46   0.00   42734.81\n",
      " 39.00   30.69   1.00   26122.09\n",
      " 50.00   20.09   3.00   11337.16\n",
      " 62.00   35.09   3.00   45705.89\n",
      " 27.00   25.08   1.00    1907.57\n",
      " 27.00   31.02   2.00    5255.97\n",
      " 59.00   29.80   3.00   11553.41\n",
      " 58.00   26.50   0.00   21775.68\n"
     ]
    }
   ],
   "source": [
    "for group, k in nogroupSynth:\n",
    "\n",
    "    obs = nogroupSynth[(group,k)] # this is k-th synth. obs. for group in question\n",
    "\n",
    "    # print synth. data for sample group: age, bmi, children, charges\n",
    "    if k < 10:\n",
    "        print(\"%6.2f %7.2f %6.2f %10.2f\" % (obs[0], obs[1], obs[2], obs[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0e6d8",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "<h2>4. Assessing quality of synthetized data</h2>\n",
    "<p>\n",
    "In particular, comparing grouping with no grouping. As an optional exercise, it would be interesting to compare the results obtained with various seeds. Or comparing grouping with no grouping by using the full set of features, using dummy variables for sex, smoking status and regions.\n",
    "<p>\n",
    "First, let's turn the data obtained so far into more traditional arrays for easy processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e02028",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_nogroupSynth = []\n",
    "arr_groupSynth = []\n",
    "arr_groupReal = []\n",
    "\n",
    "for group, k in nogroupSynth:\n",
    "    obs = nogroupSynth[(group,k)]\n",
    "    arr_nogroupSynth.append(obs)\n",
    "arr_nogroupSynth = np.transpose(arr_nogroupSynth)\n",
    "arr_nogroupSynth = np.asarray(arr_nogroupSynth, dtype = np.float64, order ='C')\n",
    "\n",
    "for group, k in groupSynth:\n",
    "    obs = groupSynth[(group,k)]\n",
    "    arr_groupSynth.append(obs)\n",
    "arr_groupSynth = np.transpose(arr_groupSynth)\n",
    "arr_groupSynth = np.asarray(arr_groupSynth, dtype = np.float64, order ='C')\n",
    "\n",
    "for group, k in groupReal:\n",
    "    obs = groupReal[(group,k)]\n",
    "    arr_groupReal.append(obs)\n",
    "arr_groupReal = np.transpose(arr_groupReal)\n",
    "arr_groupReal = np.asarray(arr_groupReal, dtype = np.float64, order ='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d495e83",
   "metadata": {},
   "source": [
    "Now let's compute correlation matrices and some averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01eb8121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corr matrix real:\n",
      " [[1.         0.10927188 0.042469   0.29900819]\n",
      " [0.10927188 1.         0.0127589  0.19834097]\n",
      " [0.042469   0.0127589  1.         0.06799823]\n",
      " [0.29900819 0.19834097 0.06799823 1.        ]]\n",
      "\n",
      "Corr matrix real group:\n",
      " [[1.         0.10927188 0.042469   0.29900819]\n",
      " [0.10927188 1.         0.0127589  0.19834097]\n",
      " [0.042469   0.0127589  1.         0.06799823]\n",
      " [0.29900819 0.19834097 0.06799823 1.        ]]\n",
      "\n",
      "Corr matrix synth. group:\n",
      " [[1.         0.10394322 0.03567531 0.22895421]\n",
      " [0.10394322 1.         0.00757671 0.19321724]\n",
      " [0.03567531 0.00757671 1.         0.05468817]\n",
      " [0.22895421 0.19321724 0.05468817 1.        ]]\n",
      "\n",
      "Corr matrix synth. nogroup:\n",
      " [[1.         0.10394322 0.03567531 0.22895421]\n",
      " [0.10394322 1.         0.00757671 0.19321724]\n",
      " [0.03567531 0.00757671 1.         0.05468817]\n",
      " [0.22895421 0.19321724 0.05468817 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "corr1 = np.corrcoef(real) \n",
    "corr2 = np.corrcoef(arr_groupReal)\n",
    "corr3 = np.corrcoef(arr_groupSynth)\n",
    "corr4 = np.corrcoef(arr_nogroupSynth)\n",
    "\n",
    "print(\"\\nCorr matrix real:\\n\",corr1)\n",
    "print(\"\\nCorr matrix real group:\\n\",corr2)\n",
    "print(\"\\nCorr matrix synth. group:\\n\",corr3)\n",
    "print(\"\\nCorr matrix synth. nogroup:\\n\",corr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fcd77",
   "metadata": {},
   "source": [
    "The first two data sets, <code>real</code> and <code>arr_groupReal</code> are of course identical, thus the correlation matrix are identical. The last two, <code>arr_groupSynth</code> and <code>arr_nogroupSynth</code> are different, yet lead to the same correlation matrix. The fit between real and synthetic is pretty good as far a replicating the correlation structure is concerned. This is true whether grouping is involved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b88f0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean real: [age, bmi, children, charges]\n",
      " [3.92070254e+01 3.06633969e+01 1.09491779e+00 1.32704223e+04]\n",
      "\n",
      "mean synth. group: [age, bmi, children, charges]\n",
      " [3.88389577e+01 3.10462786e+01 1.03336422e+00 1.30465876e+04]\n",
      "\n",
      "mean synth. nogroup: [age, bmi, children, charges]\n",
      " [3.91705999e+01 3.04771953e+01 1.11876599e+00 1.31274218e+04]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nmean real: [age, bmi, children, charges]\\n\",np.mean(arr_groupReal, axis=1))\n",
    "print(\"\\nmean synth. group: [age, bmi, children, charges]\\n\",np.mean(arr_groupSynth, axis=1))\n",
    "print(\"\\nmean synth. nogroup: [age, bmi, children, charges]\\n\",np.mean(arr_nogroupSynth, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a77a6e",
   "metadata": {},
   "source": [
    "Grouping does not lead to improvements over no grouping, regarding global statistics. Clearly, the two datasets \"group\" and \"nogroup\" show differences now; \"nogroup\" looks a tiny bit closer to the real data, but this could have happened by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0866b2",
   "metadata": {},
   "source": [
    "Now let's look at the details for a specific \"group\", to see how group significantly outperforms \"nogroup\". Again, \n",
    "the first step is to turn the data into arrays that are easy to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0761483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Obs. in group Male/Smoker/SouthEast: 55\n"
     ]
    }
   ],
   "source": [
    "group_MNS = \"male\\tyes\\tsoutheast\"\n",
    "nobs_MNS = groupCount[group_MNS]\n",
    "print(\"\\nObs. in group Male/Smoker/SouthEast:\",nobs_MNS)\n",
    "arr_group_MNS_synth = []\n",
    "arr_group_MNS_real = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f3b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(nobs_MNS):\n",
    "    obs_s = groupSynth[(group_MNS,k)]\n",
    "    obs_r = groupReal[(group_MNS,k)]\n",
    "    arr_group_MNS_synth.append(obs_s)\n",
    "    arr_group_MNS_real.append(obs_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35a06d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_group_MNS_synth = np.transpose(arr_group_MNS_synth)\n",
    "arr_group_MNS_synth = np.asarray(arr_group_MNS_synth, dtype = np.float64, order ='C')\n",
    "arr_group_MNS_real = np.transpose(arr_group_MNS_real)\n",
    "arr_group_MNS_real = np.asarray(arr_group_MNS_real, dtype = np.float64, order ='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3865e61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean real group MNS: [age, bmi, children, charges]\n",
      " [4.00545455e+01 3.36500000e+01 1.03636364e+00 3.60298394e+04]\n",
      "\n",
      "mean synth. group MNS: [age, bmi, children, charges]\n",
      " [3.72394058e+01 3.61261220e+01 9.84307691e-01 3.76027507e+04]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nmean real group MNS: [age, bmi, children, charges]\\n\",np.mean(arr_group_MNS_real, axis=1))\n",
    "print(\"\\nmean synth. group MNS: [age, bmi, children, charges]\\n\",np.mean(arr_group_MNS_synth, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f478f1",
   "metadata": {},
   "source": [
    "Without grouping, the average charges for that froup would be the average charges for the whole population, that is around \\\\$13,000. With grouping (and thus with a customized copula for that group), the average charges for that group, in the synthetic data, is \\\\$37,602. In the real data, it is \\\\$36,029. Pretty close! But very different from the global average, by an order of magnitude. That group has 55 observations, enough to get decent synthetization given the small number of features (4 features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12898e",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "<h2>5. Alternatives to Hellinger distance</h2>\n",
    "<p>\n",
    "Table-Evaluator is a Python library offering many possibilities. In Module 3, I will discuss a correlation matrix distance. Here I show how to use the Kolmogorov distance. The Hellinger distance requires to compute the empirical probability density functions (EPDF) attached to vectors of observations (features). It works great with discrete distributions (age, number of children), but it is subject to arbitrary parameters when dealing with continuous distributions (bmi, charges) unless you bin them to make them discrete. The ECDF (empirical cumulative distribution function) solves this problem and is more widespread in Python. The Kolmogorov distance is based on the ECDF. Note that it looks at individual features, not at the inter-dependencies. \n",
    "    \n",
    "The Hellinger distance in theory is easy to extend to multivariate features (thus handling the dependencies structure), but in practice this desirable property is ignored and many practitioners stick to comparing single rather than joint features. In that case, why not use Kolmogorov instead? Note that a full-fledge implementation of Hellinger for joint features comes with its own problems: a large number of multivariate bins, many too small and causing problems unless aggregated. We avoid all of this here, focusing on single features one at a time. We capture the dependencies via the correlation matrix instead, which is a good first order (linear) approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "999f26a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS distance between real and synth, for each feature (no grouping):\n",
      "feature  0: KS distance = 0.0112  P-value = 1.0000\n",
      "feature  1: KS distance = 0.0172  P-value = 0.9891\n",
      "feature  2: KS distance = 0.0164  P-value = 0.9936\n",
      "feature  3: KS distance = 0.0299  P-value = 0.5884\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "nvar = arr_nogroupSynth.shape[0]  # number of features\n",
    "print(\"KS distance between real and synth, for each feature (no grouping):\")\n",
    "for idx in range(nvar):\n",
    "    test = ks_2samp(arr_nogroupSynth[idx,:], arr_groupReal[idx,:])\n",
    "    print(\"feature %2d: KS distance = %5.4f  P-value = %5.4f\" \n",
    "        %(idx, test.statistic, test.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b157567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS distance between real and synth, for each feature (grouping):\n",
      "feature  0: KS distance = 0.0306  P-value = 0.5564\n",
      "feature  1: KS distance = 0.0419  P-value = 0.1918\n",
      "feature  2: KS distance = 0.0254  P-value = 0.7809\n",
      "feature  3: KS distance = 0.0321  P-value = 0.4944\n"
     ]
    }
   ],
   "source": [
    "nvar = arr_groupSynth.shape[0]  # number of features\n",
    "print(\"KS distance between real and synth, for each feature (grouping):\")\n",
    "for idx in range(nvar):\n",
    "    test = ks_2samp(arr_groupSynth[idx,:], arr_groupReal[idx,:])\n",
    "    print(\"feature %2d: KS distance = %5.4f  P-value = %5.4f\" \n",
    "        %(idx, test.statistic, test.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a78706",
   "metadata": {},
   "source": [
    "The Kolmogorov-Smirnov distance (0 = perfect match, 1 = worst) is really low for each feature, confirming the goodness of fit between real and synthetic data. This is further confirmed by the high P-values: a low value (close to 0) would suggest that the two distributions compared in <code>ks_2samp</code> are different. If you want a single metric, use the maximum of these 4 distances. See how it varies when using a different seed. Note that features 0, 1, 2, 3 are respectively age, bmi, number of children, and charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54eb20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS distance between real and synth for group MNS, for each feature:\n",
      "feature  0: KS distance = 0.1455  P-value = 0.6102\n",
      "feature  1: KS distance = 0.1636  P-value = 0.4565\n",
      "feature  2: KS distance = 0.0909  P-value = 0.9789\n",
      "feature  3: KS distance = 0.1091  P-value = 0.9031\n"
     ]
    }
   ],
   "source": [
    "nvar = arr_group_MNS_synth.shape[0]  # number of features\n",
    "print(\"KS distance between real and synth for group MNS, for each feature:\")\n",
    "for idx in range(nvar):\n",
    "    test = ks_2samp(arr_group_MNS_synth[idx,:], arr_group_MNS_real[idx,:])\n",
    "    print(\"feature %2d: KS distance = %5.4f  P-value = %5.4f\" \n",
    "        %(idx, test.statistic, test.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d7610",
   "metadata": {},
   "source": [
    "Same test, but this type just for one group: MNS. Again, great fit (even better!) based on P-values. The distances are a bit larger because the sample size is much smaller. \n",
    "\n",
    "Now looking at additional stats: minima and maxima for each feature, real vs synthetic (group / nogroup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d369c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum: [age bmi children charges]\n",
      "nogroup: [  18.           16.86776217    0.         1123.38907911]\n",
      "group  : [  18.           16.0368089     0.         1128.62393323]\n",
      "real   : [  18.       15.96      0.     1121.8739]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMinimum: [age bmi children charges]\")\n",
    "print(\"nogroup:\",np.min(arr_nogroupSynth, axis = 1))\n",
    "print(\"group  :\",np.min(arr_groupSynth, axis = 1))\n",
    "print(\"real   :\",np.min(arr_groupReal, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fb6ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum: [age bmi children charges]\n",
      "nogroup: [6.40000000e+01 5.29651097e+01 5.00000000e+00 5.81479332e+04]\n",
      "group  : [6.40000000e+01 5.24877257e+01 5.00000000e+00 6.11376411e+04]\n",
      "real   : [6.4000000e+01 5.3130000e+01 5.0000000e+00 6.3770428e+04]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMaximum: [age bmi children charges]\")\n",
    "print(\"nogroup:\",np.max(arr_nogroupSynth, axis = 1))\n",
    "print(\"group  :\",np.max(arr_groupSynth, axis = 1))\n",
    "print(\"real   :\",np.max(arr_groupReal, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5456c92",
   "metadata": {},
   "source": [
    "The method based on nogroup underestimates the maximum charge: \\\\$58,147 (no group) versus \\\\$61,137 (grouping) versus \\\\$63,770 (real). It also overestimates the minimum bmi: 16.86 (no group) versus 16.03 (grouping) versus 15.96 (real)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7086e",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "<h2>6. Parametric copula</h2>\n",
    "<p>\n",
    "In Module 1, I discussed how we could use a Geometric distribution of parameter $p$ instead of the empirical distribution (actually the empirical quantiles) for the number of children. You estimate $p$ on the real data. However, it leads to a less than ideal distribution, the maximum number of children being frequently over 10, despite the mean number being correct. In the real dataset, the maximim is 5.\n",
    "    <p>\n",
    "To overcome this problem, one can use a generalized geometric distribution with 2 parameters. I show how to do it in my article\n",
    "  <a href=\"https://mltechniques.com/2023/03/30/smart-grid-search-case-study-with-hybrid-zeta-geometric-distributions-and-synthetic-data/\">Smart Grid Search for Faster Hyperparameter Tuning</a>. It involves creating such a distribution, then estimating its parameters on the real data. The latter can be done using gradient descent. In my article, I use smart grid search instead. The Python implementation is in the last section in Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f22121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
