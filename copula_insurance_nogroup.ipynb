{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ec1761",
   "metadata": {},
   "source": [
    "<h1>Synthetizing the insurance Dataset</h1>\n",
    "<p>\n",
    "The method is based on copula. In this version, I work with the non-categorical features. No grouping is performed.\n",
    "The content is as follows:\n",
    "    \n",
    "<ol>\n",
    "    <li><a =\"#section1\">Imports and Reading Datasets</a>  \n",
    "    <li><a =\"#section2\">Exploratory Analysis</a>\n",
    "    <li><a =\"#section3\">Step 1: Compute correlation matrix on real data</a>\n",
    "    <li><a =\"#section4\">   Step 2: Multivariate Gaussian generation</a>\n",
    "    <li><a =\"#section5\">   Step 3 and 4: From Gaussian to uniform to target distribution</a>\n",
    "    <li><a =\"#section6\">  Synthetic Data: Snapshot</a>\n",
    "    <li><a =\"#section7\">  Gathering More Stats and Insights</a>\n",
    "    <li><a =\"#section8\">First Attempt at Parametric Copulas</a>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24118e9c",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Imports and Reading Dataset</h2><a id='section1'></a>\n",
    "<p>\n",
    "The dataset on Kaggle, <a href=\"https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset\">here</a>. \n",
    "In this notebook, I use the version on my GitHub repository,\n",
    "    <a href=\"https://github.com/VincentGranville/Main/blob/main/insurance.csv\">here</a>.\n",
    "<p>\n",
    "<b>Features: </b> age, sex, bmi, children, smoker, region, charges. \n",
    "<p>\n",
    "The last one is the response. Categorical fields (sex, smoker, regions) to be treated separately. Check out for \n",
    "outliers, \n",
    "missing values, \n",
    "values with commas inside,\n",
    "and so on. Also, do we need to transform the data?\n",
    "<p>\n",
    "<b>Exercise 1</b><br>\n",
    "Use dummy variables for categorical fields, and include them in the synthetization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ec4e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copula_insurance_nogroup.py\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "filename = 'insurance.csv' \n",
    "data = pd.read_csv(filename)\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b1228",
   "metadata": {},
   "source": [
    "Now using numerical fields only: age, bmi, children, charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6975ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = data.loc[:,\"age\"]\n",
    "bmi = data.loc[:,\"bmi\"]\n",
    "children = data.loc[:,\"children\"]\n",
    "charges = data.loc[:,\"charges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d78c7f",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "<h2>Exploratory Analysis</h2>\n",
    "<p>\n",
    "The focus is to check out what kind of distributions we are dealing with, by plotting the distribution of each feature, and scatterplots for pair of features. In particular:\n",
    "<ul>\n",
    "<li> 'age' looks uniform except for the extremes\n",
    "<li> 'bmi' looks Gaussian\n",
    "<li> 'children' looks like a geometric distribution\n",
    "<li> 'charges' is bimodal\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4455f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "axes = plt.axes()\n",
    "n_bins = 15  # n_bins = 20 produces periodic spikes, why?\n",
    "plt.hist(age, n_bins, density = True, rwidth = 0.8, color = 'orange') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e67906",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 6\n",
    "plt.hist(children, n_bins, density = True, rwidth = 0.8, color = 'orange') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37cda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 15\n",
    "plt.hist(bmi, n_bins, density = True, rwidth = 0.8, color = 'orange') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515f051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 15 # a lot smoother with 10 bins\n",
    "plt.hist(charges, n_bins, density = False, rwidth = 0.8, color = 'orange') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1426a",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "<h2>Step 1: Compute correlation matrix on real data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ba380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need correlation matrix computed on real data, for Gaussian copula\n",
    "r_data = np.stack((age, bmi, children, charges), axis = 0)\n",
    "r_corr = np.corrcoef(r_data) \n",
    "print(r_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f2186",
   "metadata": {},
   "source": [
    "Also computing the means for each feature. Not needed here, but useful to see if they make sense and get an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faaddbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mu  = [np.mean(age), np.mean(bmi), np.mean(children), np.mean(charges)]\n",
    "nobs_synth = len(age)\n",
    "print(\"Mean: %5.2f %5.2f %5.2f %6.0f\" % (r_mu[0],r_mu[1],r_mu[2],r_mu[3]))\n",
    "print(\"Nobs: %4d\" %(nobs_synth))\n",
    "zero = [0, 0, 0, 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5399b61",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "<h2>Step 2: Multivariate Gaussian generation</h2>\n",
    "<p>\n",
    "Generate multivariate Gaussian with zero mean and covariance equal to correlation matrix on real data. We generate <code>nobs_synth</code> observations. In this case, the same number as in the real data. The use of a Gaussian copula here is similar to using Gaussians for the latent data in GAN. There are alternatives, such as Frank copula, in the same way that you could use (say) uniform deviates for latent features in GAN. If some features in the real data have very thick tail, a Gaussian copula, though theoretically correct (especially if the dataset is large) may undersample extremes.\n",
    "<p>\n",
    "First, we want to control all sources of randomness for replicability and to study volatility. This is done with <code>seed</code>. When saving results, don't forget to save the seed that you used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf363f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 453\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85d8202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step to reconstruct correl structure in synth. data\n",
    "gfg = np.random.multivariate_normal(zero, r_corr, nobs_synth) \n",
    "g_age = gfg[:,0]\n",
    "g_bmi = gfg[:,1]\n",
    "g_children = gfg[:,2]\n",
    "g_charges = gfg[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041502d5",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "<h2>Step 3 and 4: From Gaussian to uniform to target distribution</h2>\n",
    "<p>\n",
    "From the correlated Gaussian with the target correlation structure, extract marginals (the features) and turn them into uniforms on [0, 1]. Then transform the uniforms into the correct target distribution: the empirical distribution of the real data, for each feature. The correlation structure is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71307be",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = []\n",
    "\n",
    "for k in range(nobs_synth):  \n",
    "\n",
    "    # Step 3: first get uniform distrib. for each feature\n",
    "    u_age = norm.cdf(g_age[k])\n",
    "    u_bmi = norm.cdf(g_bmi[k])\n",
    "    u_children = norm.cdf(g_children[k])\n",
    "    u_charges = norm.cdf(g_charges[k])\n",
    "\n",
    "    # Step 4: turn uniform into target distrib.\n",
    "    s_age = np.quantile(age, u_age)                # synthesized age \n",
    "    s_bmi = np.quantile(bmi, u_bmi)                # synthesized bmi\n",
    "    s_children = np.quantile(children, u_children) # synthesized children\n",
    "    s_charges = np.quantile(charges, u_charges)    # synthesized charges\n",
    "    s_data.append((s_age,s_bmi,s_children, s_charges))\n",
    "\n",
    "s_data = np.array(s_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183b285",
   "metadata": {},
   "source": [
    "<a id='sectionx'></a>\n",
    "<h2>Assessing Quality</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aae4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_mu = np.mean(s_data, axis=0)\n",
    "print(\"Mean Synth: %5.2f %5.2f %5.2f %6.0f\" % (s_mu[0],s_mu[1],s_mu[2],s_mu[3]))\n",
    "print(\"Mean Real : %5.2f %5.2f %5.2f %6.0f\" % (r_mu[0],r_mu[1],r_mu[2],r_mu[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89207ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_corr = np.corrcoef(np.transpose(s_data))\n",
    "print(\"Correlation, real data\\n\")\n",
    "print(r_corr)\n",
    "print(\"\\nCorrelation, synthetic data\\n\")\n",
    "print(s_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4c4d6",
   "metadata": {},
   "source": [
    "<b>Exercise 2</b><br>\n",
    "Check if pairwise feature scatter plots on real and synth. data are similar.\n",
    "<p>\n",
    "<b>Exercise 3</b><br>    \n",
    "Try with different seeds. Assess volatility of the results. Compute confidence intervals for mean age and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d658cd",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "<h2>Synthetic Data: Snapshot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eeab9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- np to pandas array\n",
    "s_data = pd.DataFrame(s_data, columns = ['age','bmi','children','charges'])\n",
    "print(s_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23b07901",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = np.transpose(r_data)\n",
    "r_data = pd.DataFrame(r_data, columns = ['age','bmi','children','charges'])\n",
    "print(r_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d6dad",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "<h2>Gathering More Stats and Insights</h2>\n",
    "<p>\n",
    "The goal is to better compare synthetic with real data, and save summary stats for future comparison with other seeds (to assess volatility) and other methods: GAN, copula with grouping, Frank copula, copula with empirical quantiles replaced by parametric distributions fit to the real data, and feature substitution to reduce algorithmic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58dd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = np.min(r_data,axis=1)\n",
    "s_min = np.min(s_data,axis=0)\n",
    "r_max = np.max(r_data,axis=1)\n",
    "s_max = np.max(s_data,axis=0)\n",
    "r_std = np.std(r_data,axis=1)\n",
    "s_std = np.std(s_data,axis=0)\n",
    "\n",
    "print(\"Min Real:\\nage\\t\\t%9.3f\\nbmi\\t\\t%9.3f\\nchildren\\t%9.3f\\ncharges\\t\\t%9.3f\\n\" \n",
    "      % (r_min[0],r_min[1],r_min[2],r_min[3]))\n",
    "print(\"Min Synth:\\nage\\t\\t%9.3f\\nbmi\\t\\t%9.3f\\nchildren\\t%9.3f\\ncharges\\t\\t%9.3f\\n\" \n",
    "      % (s_min[0],s_min[1],s_min[2],s_min[3]))\n",
    "print(\"Max Real:\\nage\\t\\t%9.3f\\nbmi\\t\\t%9.3f\\nchildren\\t%9.3f\\ncharges\\t\\t%9.3f\\n\" \n",
    "      % (r_max[0],r_max[1],r_max[2],r_max[3]))\n",
    "print(\"Max Synth:\\nage\\t\\t%9.3f\\nbmi\\t\\t%9.3f\\nchildren\\t%9.3f\\ncharges\\t\\t%9.3f\\n\" \n",
    "      % (s_max[0],s_max[1],s_max[2],s_max[3]))\n",
    "print(\"Std Real:\\nage\\t\\t%9.3f\\nbmi\\t\\t%9.3f\\nchildren\\t%9.3f\\ncharges\\t\\t%9.3f\\n\" \n",
    "      % (r_std[0],r_std[1],r_std[2],r_std[3]))\n",
    "print(\"Std Synth:\\nage\\t\\t%9.3f\\nbmi\\t\\t%9.3f\\nchildren\\t%9.3f\\ncharges\\t\\t%9.3f\" \n",
    "      % (s_std[0],s_std[1],s_std[2],s_std[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c373c",
   "metadata": {},
   "source": [
    "<b>Exercise 4</b><br>\n",
    "Add 25- and 75-percentiles, both for real and synthetic.\n",
    "<p>\n",
    "<b>Exercise 5</b><br> \n",
    "Compute distance between statistical summaries on real and synthetized data. To do so, transform real data so that each feature has zero mean and unit variance. Make the comparison between standardized data and the synth data produced on the standardized version of the real data. Then try different seeds, and see which ones provide best fit.\n",
    "<p>\n",
    "<b>Exercise 6</b><br>\n",
    "In addition to the above statistics in Exercise 5, also include the correlation matrices. No need to transform the data here: these matrices are invariant under linear transformations. To be discussed in the GAN module. \n",
    "<p>\n",
    "<b>Exercise 7</b><br>\n",
    "Instead of summary stats, use Hellinger distance for comparison purposes: (1) feature-wise, (2) max or average Hellinger across features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306a067",
   "metadata": {},
   "source": [
    "<b>Conclusions</b><p>\n",
    "The copulas do a great job at replicating the correlation structure and marginal distributions. They work with both ordinal and continuous features, producing ordinal values for ordinal features. Replication of the results is easy thanks to\n",
    "<code>seed</code>. Also, it is a very fast technique. We will see that copulas, unlike GANs, are not great at replicating non linear structures (correlation is a linear structure). Also, unless using parametric distributions for the quantiles, you can't sample outside the range of observations (min, max) in the real data. This issue is true for all features, but easy to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143e650",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "<h2>First Attempt at Parametric Copulas</h2>\n",
    "<p>\n",
    "Let's replace the empirical quantiles for 'children' by quantiles of a geometric distribution of parameter <em>p</em>. \n",
    "The parameter is estimated on the real data, as the inverse of the mean for the feature in question. The\n",
    "feature <code>s_children</code> obtained in the synthetic data will have a\n",
    "sister feature <code>s_param_children</code>. See what happens when we switch them. Are the results worse? Are we able to sample outside the range? (that was the purpose). Likewise, we could use a GMM (Gaussian mixture model) for the bimodal 'charges', preferably after a log-transform to make sure we do not generate negative charges in the synthetic data (and typically, expenses may follow a log-normal rather than normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac19f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
